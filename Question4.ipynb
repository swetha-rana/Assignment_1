Open In Colab
!pip install wandb
Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.10)
Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)
Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)
Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)
Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)
Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)
Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)
Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)
Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)
Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)
Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)
Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)
Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.6)
Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)
Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)
Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)
Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)
Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)
Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)
import wandb
wandb.init(project="Assignment-1", entity="swe-rana")
wandb: Currently logged in as: swe-rana (use `wandb login --relogin` to force relogin)
Syncing run grateful-glade-187 to Weights & Biases (docs).
import wandb
wandb.init(project="Assignment-1", entity="swe-rana")

from keras.datasets import fashion_mnist
import matplotlib.pyplot as plt
import random
(X_train,y_train),(X_test,y_test) = fashion_mnist.load_data()
label0=[]
label1=[]
label2=[]
label3=[]
label4=[]
label5=[]
label6=[]
label7=[]
label8=[]
label9=[]
for i in range(len(y_train)):
    if (y_train[i]==0):
        label0.append(i)
    if (y_train[i]==1):
        label1.append(i)
    if (y_train[i]==2):
        label2.append(i)
    if (y_train[i]==3):
        label3.append(i)
    if (y_train[i]==4):
        label4.append(i)
    if (y_train[i]==5):
        label5.append(i)
    if (y_train[i]==6):
        label6.append(i)
    if (y_train[i]==7):
        label7.append(i)
    if (y_train[i]==8):
        label8.append(i)
    if (y_train[i]==9):
        label9.append(i)
Class_names=(label0,label1,label2,label3,label4,label5,label6,label7,label8,label9)    
data = ("T-shirt/top","Trouser","Pullover","Dress","Coat","Sandal","Shirt","Sneaker","Bag","Ankle boot")
rows=2
columns =5
fig = plt.figure(figsize=(10, 7))
for i,j in zip(range(1, columns*rows +1),range(0,10)):
        num = random.choice(Class_names[j])
        wandb.log({"images": [wandb.Image(X_train[num],caption=data[j])]})
        fig.add_subplot(rows, columns, i)
        plt.imshow(X_train[num],cmap ="gray")
        plt.axis('off')
        plt.title(data[j])
wandb.log({"chart": plt})
Finishing last run (ID:3orq3prt) before initializing another...

Waiting for W&B process to finish, PID 2237... (success).
VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\r'), FloatProgress(value=1.0, max=1.0)â€¦
Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)
Synced sleek-darkness-188: https://wandb.ai/swe-rana/Assignment-1/runs/3orq3prt
Find logs at: ./wandb/run-20220225_175827-3orq3prt/logs
Successfully finished last run (ID:3orq3prt). Initializing new run:
Syncing run charmed-armadillo-189 to Weights & Biases (docs).

from keras.datasets import fashion_mnist
import numpy as np
import matplotlib.pyplot as plt
from numpy import exp
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


(t_train_x_orig,t_train_y),(test_x_orig,test_y)= fashion_mnist.load_data()

train_x_orig,x_val,train_y,y_val=train_test_split(t_train_x_orig,t_train_y,test_size=0.1,random_state = 43)

m_train = train_x_orig.shape[0]
num_px = train_x_orig.shape[1]
m_test = test_x_orig.shape[0]
train_y = train_y.reshape(1, len(train_y))
test_y = test_y.reshape(1, len(test_y))
y_val = y_val.reshape(1, len(y_val))
train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T
x_val_flatten = x_val.reshape(x_val.shape[0],-1).T
test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T
no_of_class=10

train_x = train_x_flatten/255
test_x = test_x_flatten/255
x_val = x_val_flatten/255
#layers_dims = [len(train_x),256,128,no_of_class]
onehot_encoded = list()

for i in range(train_y.shape[1]):
    c=train_y[:,i][0]
    letter = [0 for _ in range(no_of_class)]
    letter[c] = 1
    onehot_encoded.append(letter)

N=np.array(onehot_encoded)
Y=N.reshape(no_of_class,train_y.shape[1])
for i in range(0,train_y.shape[1]):
      Y[:,i] = N[i]

onehot_encoded_y_val = list()

for i in range(y_val.shape[1]):
    c=y_val[:,i][0]
    letter = [0 for _ in range(no_of_class)]
    letter[c] = 1
    onehot_encoded_y_val.append(letter)


M=np.array(onehot_encoded_y_val)
Y_val=M.reshape(no_of_class,y_val.shape[1])
for i in range(0,y_val.shape[1]):
      Y_val[:,i] = N[i]


#layers_dims = [len(train_x),256,128,no_of_class]


def initialize_parameters(layers_dims,initialization):    
    np.random.seed(3)
    parameters = {}
    L = len(layers_dims)            # number of layers in the network

    for l in range(1, L):
        if initialization == 'Normal':
            parameters["W"+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01
        elif initialization == 'Uniform':
            parameters["W"+str(l)] = np.random.rand(layers_dims[l], layers_dims[l-1]) * 0.01
        elif initialization == 'Xavier':
            parameters["W"+str(l)]= np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2/(layers_dims[l]+layers_dims[l-1]))
        parameters['b' + str(l)] =  np.zeros((layers_dims[l], 1))
    return parameters

def prev_updates(layers_dims):
        previous_updates = {}
        L = len(layers_dims)            # number of layers in the network
        for l in range(1, L):
            previous_updates["W"+str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))
            previous_updates["b"+str(l)] = np.zeros((layers_dims[l], 1))
                    
        return previous_updates


def feed_forward(A, W, b):

    Z =np.dot(W, A) + b
    cache = (A, W, b)
    
    return Z, cache 





def sigmoid(Z):
    A = 1/(1+np.exp(-Z))
    cache = Z
    return A, cache

def relu(Z):
    
    A = np.maximum(0,Z)
    
    assert(A.shape == Z.shape)
    
    cache = Z 
    return A, cache

def tanh(Z):
    A = np.tanh(Z)
    cache = Z
    return A,cache

def tanh_backward(dA,cache):
    Z=cache
    t = np.tanh(Z)
    dZ = 1 - (t**2)
    return dZ

def sigmoid_backward(dA, cache):
   
    Z = cache
    s = 1/(1+np.exp(-Z))
    dZ = dA * s * (1-s)
    
    return dZ

def relu_backward(dA, cache):

    
    Z = cache
    dZ = np.array(dA, copy=True) 
    
    dZ[Z <= 0] = 0
     
    return dZ
    
def softmax(n):
 	e = exp(n)
 	return e / e.sum()
 	
 	
 	
def activation_forward(A_prev, W, b, activation):
    if activation == "sigmoid":
        Z, linear_cache  = feed_forward(A_prev, W, b)
        A, activation_cache = sigmoid(Z)

    if activation == "tanh":
        Z, linear_cache  = feed_forward(A_prev, W, b)
        A, activation_cache = tanh(Z)
    
    if activation == "relu":
        
        Z, linear_cache = feed_forward(A_prev, W, b)
        A, activation_cache = relu(Z)

    cache = (linear_cache, activation_cache)

    return A, cache


def L_model_forward(X, parameters):
    
    caches = []
    A = X
    L = len(parameters) // 2                  # number of layers in the neural network
    
    for l in range(1, L):
        A_prev = A 
        A, cache = activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation)
        caches.append(cache)
        
    AL, cache = activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation)
    caches.append(cache)
            
    return AL, caches
    
#backpropagtion    
def linear_backward(dZ, cache):
    A_prev, W, b = cache
    m = A_prev.shape[1]
    dW = 1./m*np.dot(dZ, A_prev.T)
    db = 1./m*np.sum(dZ, axis = 1, keepdims=True)
    dA_prev = np.dot(W.T, dZ)
    return dA_prev, dW, db

    
    
def activation_backward(dA, cache, activation):
    linear_cache, activation_cache = cache
    
    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
    
    if activation == "tanh":
        dZ = tanh_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

        
    if activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    
    return dA_prev, dW, db    

def L_model_backward(Y,AL, caches):
    grads = {}
    L = len(caches) # the number of layers
    m = AL.shape[1]
    Y = Y.reshape(AL.shape) 
    # Initializing the backpropagation
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    
    current_cache = caches[L-1]
    grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = activation_backward(dAL, current_cache, "sigmoid")
    
    for l in reversed(range(L-1)):
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = activation_backward(grads["dA" + str(l + 2)],  current_cache, "relu")
        grads["dA" + str(l + 1)] = dA_prev_temp
        grads["dW" + str(l + 1)] = dW_temp
        grads["db" + str(l + 1)] = db_temp

    return grads
def update_parameters(parameters, grads, learning_rate,lamda):
    
    L = len(parameters) // 2 # number of layers in the neural network

    for l in range(L):
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l + 1)] 
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l + 1)]
        
    return parameters
    
#stochastic gradient    


def stochastic_gradient(X, Y, layers_dims, learning_rate,num_epochs,lamda,initialisation,loss):
          z_acc = []
          val_acc = []
          Train_loss = []
          Val_loss = []
          parameters = initialize_parameters(layers_dims,initialisation)
          print("in stochastic")
          for j in range(0,num_epochs):
            for i in range(0,iterations_bat):
                start = i*batch_size
                end = start+batch_size
                AL, caches = L_model_forward(X[:,start:end], parameters)
                grads = L_model_backward(Y[:,start:end],AL, caches)
                parameters = update_parameters(parameters, grads, learning_rate,lamda)
            z_pred_1, caches = L_model_forward(train_x, parameters)
            z_pred = np.argmax(z_pred_1,axis = 0)
            zyy = train_y.flatten()
            z_acc.append(accuracy_score(zyy,z_pred))
            print("Train accuracy",z_acc) 
            val_pred, caches = L_model_forward(x_val, parameters)
            val_prediction = np.argmax(val_pred,axis = 0)
            val_y_flat = y_val.flatten()
            val_acc.append(accuracy_score(val_y_flat,val_prediction))

            if loss =="cross_entropy" :
                Train_loss.append(cross_entropy_loss(train_x,Y,parameters))
                Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))
            elif loss =="MSE" :
                 Train_loss.append(MSE(train_x,Y,parameters))
                 Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))
          return parameters,z_acc,val_acc,Train_loss,Val_loss
#momentum gradient descent optimizer
def momentum(X,Y,layers_dims,learning_rate,beta,num_epochs,initialisation,loss):
    z_acc = []
    val_acc = []
    
    Train_loss = []
    Val_loss = []
    parameters = initialize_parameters(layers_dims,initialisation)
    previous_updates =prev_updates(layers_dims)
    L = len(parameters) // 2 # number of layers in the neural network
    for j in range(0,num_epochs):
        for i in range(0,iterations_bat):
            start = i*batch_size
            end = start+batch_size
            AL, caches = L_model_forward(X[:,start:end], parameters)
            grads = L_model_backward(Y[:,start:end],AL, caches)
                                   
            for l in range(1, L + 1):
                previous_updates["W"+str(l)] = (beta*previous_updates["W"+str(l)]) + ((1-beta)*grads["dW" + str(l)])
                parameters["W" + str(l)] = parameters["W" + str(l)] - (learning_rate*previous_updates["W"+str(l)])
                
                previous_updates["b"+str(l)] = (beta*previous_updates["b"+str(l)]) + ((1-beta)*grads["db" + str(l)])
                parameters["b" + str(l)] = parameters["b" + str(l)] - (learning_rate*previous_updates["b"+str(l)])
            
          
        z_pred_1, caches = L_model_forward(train_x, parameters)
        z_pred = np.argmax(z_pred_1,axis = 0)
        zyy = train_y.flatten()
        z_acc.append(accuracy_score(zyy,z_pred))
        print("Train accuracy",z_acc) 
        val_pred, caches = L_model_forward(x_val, parameters)
        val_prediction = np.argmax(val_pred,axis = 0)
        val_y_flat = y_val.flatten()
        val_acc.append(accuracy_score(val_y_flat,val_prediction))
        print("validation accuracy",val_acc) 
        if loss =="cross_entropy" :
            Train_loss.append(cross_entropy_loss(train_x,Y,parameters))
            Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))
        elif loss =="MSE" :
             Train_loss.append(MSE(train_x,Y,parameters))
             Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))

    return parameters, previous_updates,z_acc,val_acc,Train_loss,Val_loss
# rmsprop optimizer
def rmsprop(X,Y,layers_dims,learning_rate,beta,num_epochs,initialisation,loss):
    z_acc = []
    val_acc = []
    Train_loss = []
    Val_loss = []
    
    parameters = initialize_parameters(layers_dims,initialisation)
    previous_updates =prev_updates(layers_dims)
    for j in range(0,num_epochs):
        for i in range(0,iterations_bat):
           
           start = i*batch_size
           end = start+batch_size
           AL, caches = L_model_forward(X[:,start:end], parameters)

           grads = L_model_backward(AL, Y[:,start:end], caches)

           delta = 1e-6 
            
           L = len(parameters) // 2 
        
           for l in range(1, L + 1):
                vdw = beta*previous_updates["W" + str(l)] + (1-beta)*np.multiply(grads["dW" + str(l)],grads["dW" + str(l)])
                vdb = beta*previous_updates["b" + str(l)] + (1-beta)*np.multiply(grads["db" + str(l)],grads["db" + str(l)])
        
                parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * grads["dW" + str(l)] / (np.sqrt(vdw)+delta)
                parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * grads["db" + str(l)] / (np.sqrt(vdb)+delta)
        
                previous_updates["W" + str(l)] = vdw
                previous_updates["b" + str(l)] = vdb
           
        z_pred_1, caches = L_model_forward(train_x, parameters)
        z_pred = np.argmax(z_pred_1,axis = 0)
        zyy = train_y.flatten()
        z_acc.append(accuracy_score(zyy,z_pred))
        print("Train accuracy",z_acc) 
        val_pred, caches = L_model_forward(x_val, parameters)
        val_prediction = np.argmax(val_pred,axis = 0)
        val_y_flat = y_val.flatten()
        val_acc.append(accuracy_score(val_y_flat,val_prediction))
        print("validation accuracy",val_acc) 
        if loss =="cross_entropy" :
            Train_loss.append(cross_entropy_loss(train_x,Y,parameters))
            Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))
        elif loss =="MSE" :
             Train_loss.append(MSE(train_x,Y,parameters))
             Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))        

    return parameters, previous_updates,z_acc,val_acc,Train_loss,Val_loss

def adam(X,Y,layers_dims,v,m,t,learning_rate,beta,num_epochs,initialisation,loss):
    z_acc = []
    val_acc = []
    Train_loss = []
    Val_loss = []

    parameters = initialize_parameters(layers_dims,initialisation)
    for j in range(0,num_epochs):
            for i in range(0,iterations_bat):
                start = i*batch_size
                end = start+batch_size
                AL, caches = L_model_forward(X[:,start:end], parameters)
                grads = L_model_backward(Y[:,start:end],AL, caches)
                
                L = len(parameters) // 2 # number of layers in the neural network
                beta1 = 0.9
                beta2 = 0.999
                epsilon = 1e-8
            
                for l in range(1, L+1):
                    mdw = beta1*m["W"+str(l)] + (1-beta1)*grads["dW"+str(l)]
                    vdw = beta2*v["W"+str(l)] + (1-beta2)*np.square(grads["dW"+str(l)])
                    mw_hat = mdw/(1.0 - beta1**t)
                    vw_hat = vdw/(1.0 - beta2**t)
            
                    parameters["W"+str(l)] = parameters["W"+str(l)] - (learning_rate * mw_hat)/np.sqrt(vw_hat + epsilon)
            
                    mdb = beta1*m["b"+str(l)] + (1-beta1)*grads["db"+str(l)]
                    vdb = beta2*v["b"+str(l)] + (1-beta2)*np.square(grads["db"+str(l)])
                    mb_hat = mdb/(1.0 - beta1**t)
                    vb_hat = vdb/(1.0 - beta2**t)
            
                    parameters["b"+str(l)] = parameters["b"+str(l)] - (learning_rate * mb_hat)/np.sqrt(vb_hat + epsilon)
            
                    v["dW"+str(l)] = vdw
                    m["dW"+str(l)] = mdw
                    v["db"+str(l)] = vdb
                    m["db"+str(l)] = mdb
            
                t = t + 1 # timestep   
            
            z_pred_1, caches = L_model_forward(train_x, parameters)
            z_pred = np.argmax(z_pred_1,axis = 0)
            zyy = train_y.flatten()
            z_acc.append(accuracy_score(zyy,z_pred))
            print("Train accuracy",z_acc) 
            val_pred, caches = L_model_forward(x_val, parameters)
            val_prediction = np.argmax(val_pred,axis = 0)
            val_y_flat = y_val.flatten()
            val_acc.append(accuracy_score(val_y_flat,val_prediction))
            print("validation accuracy",val_acc) 

            if loss =="cross_entropy" :
                Train_loss.append(cross_entropy_loss(train_x,Y,parameters))
                Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))
            elif loss =="MSE" :
                 Train_loss.append(MSE(train_x,Y,parameters))
                 Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))

    return parameters,v,m,t,z_acc,val_acc,Train_loss,Val_loss

def Nadam(X,Y,layers_dims,m,v,t,learning_rate,beta,num_epochs,initialisation,loss):
    
    z_acc = []
    val_acc = []
    Train_loss = []
    Val_loss = []


    parameters = initialize_parameters(layers_dims,initialisation)
    previous_updates = v
    L = len(parameters )//2
    for j in range(0,num_epochs):
        for l in range(1, L+1):
            parameters ["W"+str(l)] = parameters ["W"+str(l)] - beta*previous_updates["W"+str(l)]
            parameters ["b"+str(l)] = parameters ["b"+str(l)] - beta*previous_updates["b"+str(l)]
        for i in range(0,iterations_bat):
            start = i*batch_size
            end = start+batch_size
            AL, caches = L_model_forward(X[:,start:end], parameters)
            grads = L_model_backward( Y[:,start:end],AL,caches)
            
            L = len(parameters) // 2 # number of layers in the neural network
            beta1 = 0.9
            beta2 = 0.999
            epsilon = 1e-8
        
            for l in range(1, L+1):
                mdw = beta1*m["W"+str(l)] + (1-beta1)*grads["dW"+str(l)]
                vdw = beta2*v["W"+str(l)] + (1-beta2)*np.square(grads["dW"+str(l)])
                mw_hat = mdw/(1.0 - beta1**t)
                vw_hat = vdw/(1.0 - beta2**t)
        
                parameters["W"+str(l)] = parameters["W"+str(l)] - (learning_rate * mw_hat)/np.sqrt(vw_hat + epsilon)
        
                mdb = beta1*m["b"+str(l)] + (1-beta1)*grads["db"+str(l)]
                vdb = beta2*v["b"+str(l)] + (1-beta2)*np.square(grads["db"+str(l)])
                mb_hat = mdb/(1.0 - beta1**t)
                vb_hat = vdb/(1.0 - beta2**t)
        
                parameters["b"+str(l)] = parameters["b"+str(l)] - (learning_rate * mb_hat)/np.sqrt(vb_hat + epsilon)
        
                v["dW"+str(l)] = vdw
                m["dW"+str(l)] = mdw
                v["db"+str(l)] = vdb
                m["db"+str(l)] = mdb
        
            t = t + 1 # timestep            

        z_pred_1, caches = L_model_forward(train_x, parameters)
        z_pred = np.argmax(z_pred_1,axis = 0)
        zyy = train_y.flatten()
        z_acc.append(accuracy_score(zyy,z_pred))
        print("Train accuracy",z_acc) 
        val_pred, caches = L_model_forward(x_val, parameters)
        val_prediction = np.argmax(val_pred,axis = 0)
        val_y_flat = y_val.flatten()
        val_acc.append(accuracy_score(val_y_flat,val_prediction))
        print("validation accuracy",val_acc) 
        if loss =="cross_entropy" :
            Train_loss.append(cross_entropy_loss(train_x,Y,parameters))
            Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))
        elif loss =="MSE" :
             Train_loss.append(MSE(train_x,Y,parameters))
             Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))

    return parameters,z_acc,val_acc,Train_loss,Val_loss

def nesterov(X,Y,learning_rate,beta,previous_updates,num_epochs,initialisation,loss):
    z_acc = []
    val_acc = []        
    Train_loss = []
    Val_loss = []

    parameters=initialize_parameters(layers_dims,initialisation)
    L = len(parameters)//2
    for j in range(0,num_epochs):
        for l in range(1, L+1):
            parameters["W"+str(l)] = parameters["W"+str(l)] - beta*previous_updates["W"+str(l)]
            parameters["b"+str(l)] = parameters["b"+str(l)] - beta*previous_updates["b"+str(l)]
        for i in range(0,iterations_bat):
            start = i*batch_size
            end = start+batch_size    
            AL, caches = L_model_forward(X[:,start:end], parameters)
            grads = L_model_backward( Y[:,start:end],AL,caches)
            
            L = len(parameters) // 2 # number of layers in the neural network
           
            for l in range(1, L + 1):
                previous_updates["W"+str(l)] = beta*previous_updates["W"+str(l)] + (1-beta)*grads["dW" + str(l)]
                parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate*previous_updates["W"+str(l)]
                
                previous_updates["b"+str(l)] = beta*previous_updates["b"+str(l)] + (1-beta)*grads["db" + str(l)]
                parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate*previous_updates["b"+str(l)]
             
        z_pred_1, caches = L_model_forward(train_x, parameters)
        z_pred = np.argmax(z_pred_1,axis = 0)
        zyy = train_y.flatten()
        z_acc.append(accuracy_score(zyy,z_pred))
        print("Train accuracy",z_acc) 
        val_pred, caches = L_model_forward(x_val, parameters)
        val_prediction = np.argmax(val_pred,axis = 0)
        val_y_flat = y_val.flatten()
        val_acc.append(accuracy_score(val_y_flat,val_prediction))
        print("validation accuracy",val_acc) 
        
        if loss =="cross_entropy" :
            Train_loss.append(cross_entropy_loss(train_x,Y,parameters))
            Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))
        elif loss =="MSE" :
             Train_loss.append(MSE(train_x,Y,parameters))
             Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))        

    return parameters,z_acc,val_acc,Train_loss,Val_loss            


def MSE(X,Y,parameters):
    AL, caches = L_model_forward(X, parameters)
    sm=[]
    for i in range(AL.shape[1]):
        n=AL[:,i]
        u=softmax(n)
        sm.append(u)
    p=np.array(sm) 
    v=p.T
    Loss = (1/2) * np.sum((Y-v)**2)/train_x.shape[1]
    return Loss


def cross_entropy_loss(X,Y,parameters):
    AL, caches = L_model_forward(X, parameters)
    sm=[]
    for i in range(AL.shape[1]):
        n=AL[:,i]
        u=softmax(n)
        sm.append(u)
    p=np.array(sm) 
    v=p.T
    val=-np.sum(Y*(np.log(v)))
    val=val/train_x.shape[1]    
    return val
    
global batch_size
global iterations_bat
global activation
global layers_dims
def train_network(learning_rate,num_epochs,optimizer,loss,init_param,hid_layer_sizes,bat_size,activate_func):
    t = 1
    beta = 0.9
    lamda = 0.0005
    hid_layer_sizes
    no_of_class = [len(np.unique(train_y))]
    global layers_dims
    layers_dims = [len(train_x)] + hid_layer_sizes +no_of_class
    global activation
    global batch_size
    global iterations_bat
    activation = activate_func
    batch_size = bat_size
    iterations_bat = int(train_x.shape[1]/batch_size) 

    print(layers_dims)
#    loss = "cross_entropy"
    initialisation = init_param #"Xavier"
    gd_optimizer = optimizer
    previous_updates = prev_updates(layers_dims)
    if(gd_optimizer == "stochastic_gradient"):
        parameters,z_acc,val_acc,Train_loss,Val_loss =stochastic_gradient(train_x, Y, layers_dims, learning_rate,num_epochs,lamda,initialisation,loss)

    if(gd_optimizer == "momentum"):
        parameters,previous_updates,z_acc,val_acc,Train_loss,Val_loss=momentum(train_x,Y,layers_dims,learning_rate,beta,num_epochs,initialisation,loss)

    if(gd_optimizer == "rmsprop"):
        parameters, previous_updates,z_acc,val_acc,Train_loss,Val_loss=rmsprop(train_x,Y,layers_dims,learning_rate,beta,num_epochs,initialisation,loss)

    if(gd_optimizer == "Adam"):
        parameters,v,m,t,z_acc,val_acc,Train_loss,Val_loss=adam(train_x,Y,layers_dims,previous_updates,previous_updates,t,learning_rate,beta,num_epochs,initialisation,loss)

    if(gd_optimizer == "Nadam"): 
        parameters,z_acc,val_acc,Train_loss,Val_loss=Nadam(train_x,Y,layers_dims,previous_updates,previous_updates,t,learning_rate,beta,num_epochs,initialisation,loss)

    if(gd_optimizer == "nesterov"):
        parameters,z_acc,val_acc,Train_loss,Val_loss=nesterov(train_x,Y,learning_rate,beta,previous_updates,num_epochs,initialisation,loss)

    train_accuracy = [element * 100 for element in z_acc]
    val_accuracy = [element * 100 for element in val_acc]
    # wandb.log({'train_loss':Train_loss})         
    # wandb.log({'train_accuracy':train_accuracy})
    # wandb.log({'val_accuracy':val_accuracy})
    # wandb.log({'val_loss':Val_loss})
    # wandb.log({'num_epochs':num_epochs})
    #for zi in range(len(train_accuracy)):
    #    print(zi)
    #    wandb.log({"train_acc": train_accuracy, "validation_accuracy": val_accuracy, "train_loss": Train_loss, "validation cost": Val_loss, 'epoch': zi+1})

    print("val accuracy", val_accuracy)
    print("train accuracy", train_accuracy)
    return val_accuracy,val_accuracy,train_accuracy,Train_loss,Val_loss        

#train_netwrok(0.001,5,"Adam","cross-entropy","Xavier",[256,128],100,"sigmoid")
sweep_config = {
    'method': 'random',
    'metric': {
      'name': 'validation_accuracy',
      'goal': 'maximize' 
    },
    'parameters': {
        'learning_rate': {
            'values': [0.01,0.001]
        },
        'num_epochs': {
            'values': [10,15,20]
        },
        'optimizer': {
            'values': ['Adam','Nadam','momentum','stochastic_gradient','rmsprop','nesterov']
        },
        'loss': {
            'values': ['cross_entropy','MSE']
        },
        'init_param': {
            'values': ['Xavier','Normal']
        },
        'hid_layer_sizes': {
            'values': [[256,128], [256,128,64],[128,128,128]]
        }, 
        'bat_size': {
            'values': [16,32,48]
        }, 
        'activate_func': {
            'values': ['sigmoid','relu','tanh']
        }, 
    }
}
#sweep_id = wandb.sweep(sweep_config, project="Assignment-1", entity="swe-rana")
def train():
    config_defaults = {
        'learning_rate': 0.01,
        'num_epochs': 5,
        'optimizer': 'Adam',
        'loss': 'cross_entropy',
        'init_param': 'Xavier',
        'hid_layer_sizes' : [256,128],
        'bat_size':50,
        'activate_func': 'sigmoid'
    }

    wandb.init(config=config_defaults)
    config = wandb.config
    learning_rate = config.learning_rate
    num_epochs = config.num_epochs
    optimizer = config.optimizer
    loss = config.loss
    init_param = config.init_param
    hid_layer_sizes = config.hid_layer_sizes
    bat_size = config.bat_size
    activate_func = config.activate_func
    run_name = "hl_{}_bs_{}_ac_{}_initparam_{}_op_{}_ep_{}_lr_{}".format(hid_layer_sizes,bat_size, activate_func, init_param, optimizer,num_epochs,learning_rate)

    val_accuracy,val_accuracy,train_accuracy,Train_loss,Val_loss=train_network(learning_rate,num_epochs,optimizer,loss,init_param,hid_layer_sizes,bat_size,activate_func)
    for zi in range(len(train_accuracy)):
        print(zi)
        wandb.log({'train_loss':Train_loss[zi]})         
        wandb.log({'train_accuracy':train_accuracy[zi]})
        wandb.log({'val_accuracy':val_accuracy[zi]})
        wandb.log({'val_loss':Val_loss[zi]})
        wandb.log({'num_epochs':zi+1})
        #wandb.log({"train_acc": train_accuracy, "validation_accuracy": val_accuracy, "train_loss": Train_loss, "validation cost": Val_loss, 'epoch': zi+1})

    wandb.run.name = run_name
    wandb.run.save()
    wandb.run.finish()
import warnings
sweep_id = wandb.sweep(sweep_config, project="Assignment-1", entity="swe-rana")
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)
wandb.agent(sweep_id, train, count=100)
validation accuracy [0.09633333333333334, 0.09633333333333334, 0.09633333333333334, 0.09633333333333334, 0.09633333333333334, 0.09633333333333334]
wandb: Ctrl + C detected. Stopping sweep.
