{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfJDAty5YANH+Nzsvwu1zU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swetha-rana/Assignment_1/blob/main/Question4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhnKzTaVgKRx",
        "outputId": "0b02b0e1-9908-463a-d49b-bc79a2eed112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.6-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.0 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=bf3427b04315ce683653b046c99236ef3ddebdb788802347afb542002aeb25cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.6 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"Assignment-1\", entity=\"swe-rana\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "FmouXlF5gt-S",
        "outputId": "bbded880-7d38-491d-ad28-c53d21ec04ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/swe-rana/Assignment-1/runs/4wsfaask\" target=\"_blank\">valiant-sun-12</a></strong> to <a href=\"https://wandb.ai/swe-rana/Assignment-1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f6a876a26d0>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/swe-rana/Assignment-1/runs/4wsfaask?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "(X_train,y_train),(X_test,y_test) = fashion_mnist.load_data()\n",
        "label0=[]\n",
        "label1=[]\n",
        "label2=[]\n",
        "label3=[]\n",
        "label4=[]\n",
        "label5=[]\n",
        "label6=[]\n",
        "label7=[]\n",
        "label8=[]\n",
        "label9=[]\n",
        "for i in range(len(y_train)):\n",
        "    if (y_train[i]==0):\n",
        "        label0.append(i)\n",
        "    if (y_train[i]==1):\n",
        "        label1.append(i)\n",
        "    if (y_train[i]==2):\n",
        "        label2.append(i)\n",
        "    if (y_train[i]==3):\n",
        "        label3.append(i)\n",
        "    if (y_train[i]==4):\n",
        "        label4.append(i)\n",
        "    if (y_train[i]==5):\n",
        "        label5.append(i)\n",
        "    if (y_train[i]==6):\n",
        "        label6.append(i)\n",
        "    if (y_train[i]==7):\n",
        "        label7.append(i)\n",
        "    if (y_train[i]==8):\n",
        "        label8.append(i)\n",
        "    if (y_train[i]==9):\n",
        "        label9.append(i)\n",
        "Class_names=(label0,label1,label2,label3,label4,label5,label6,label7,label8,label9)    \n",
        "data = (\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\")\n",
        "rows=2\n",
        "columns =5\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "for i,j in zip(range(1, columns*rows +1),range(0,10)):\n",
        "        num = random.choice(Class_names[j])\n",
        "        #wandb.log({\"images\": [wandb.Image(X_train[num],caption=data[j])]})\n",
        "        fig.add_subplot(rows, columns, i)\n",
        "        plt.imshow(X_train[num],cmap =\"gray\")\n",
        "        plt.axis('off')\n",
        "        plt.title(data[j])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "UBGY9t2ehGvq",
        "outputId": "33b276ab-d38a-4da2-e098-4b1b7fc72635"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize' \n",
        "    },\n",
        "    'parameters': {\n",
        "        'num_epochs': {\n",
        "            'values': [5,10]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [0.01,0.001]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam','momentum']\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TVP3dHHKPStD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import exp\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "(t_train_x_orig,t_train_y),(test_x_orig,test_y)= fashion_mnist.load_data()\n",
        "\n",
        "train_x_orig,x_val,train_y,y_val=train_test_split(t_train_x_orig,t_train_y,test_size=0.1,random_state = 43)\n",
        "\n",
        "m_train = train_x_orig.shape[0]\n",
        "num_px = train_x_orig.shape[1]\n",
        "m_test = test_x_orig.shape[0]\n",
        "train_y = train_y.reshape(1, len(train_y))\n",
        "test_y = test_y.reshape(1, len(test_y))\n",
        "y_val = y_val.reshape(1, len(y_val))\n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
        "x_val_flatten = x_val.reshape(x_val.shape[0],-1).T\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "no_of_class=10\n",
        "\n",
        "train_x = train_x_flatten/255\n",
        "test_x = test_x_flatten/255\n",
        "x_val = x_val_flatten/255\n",
        "#layers_dims = [len(train_x),256,128,no_of_class]\n",
        "onehot_encoded = list()\n",
        "\n",
        "for i in range(train_y.shape[1]):\n",
        "    c=train_y[:,i][0]\n",
        "    letter = [0 for _ in range(no_of_class)]\n",
        "    letter[c] = 1\n",
        "    onehot_encoded.append(letter)\n",
        "\n",
        "N=np.array(onehot_encoded)\n",
        "Y=N.reshape(no_of_class,train_y.shape[1])\n",
        "for i in range(0,train_y.shape[1]):\n",
        "      Y[:,i] = N[i]\n",
        "\n",
        "onehot_encoded_y_val = list()\n",
        "\n",
        "for i in range(y_val.shape[1]):\n",
        "    c=y_val[:,i][0]\n",
        "    letter = [0 for _ in range(no_of_class)]\n",
        "    letter[c] = 1\n",
        "    onehot_encoded_y_val.append(letter)\n",
        "\n",
        "\n",
        "M=np.array(onehot_encoded_y_val)\n",
        "Y_val=M.reshape(no_of_class,y_val.shape[1])\n",
        "for i in range(0,y_val.shape[1]):\n",
        "      Y_val[:,i] = N[i]\n",
        "\n",
        "\n",
        "layers_dims = [len(train_x),256,128,no_of_class]\n",
        "\n",
        "\n",
        "def initialize_parameters(layers_dims,initialization):    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layers_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        if initialization == 'Normal':\n",
        "            parameters[\"W\"+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01\n",
        "        elif initialization == 'Uniform':\n",
        "            parameters[\"W\"+str(l)] = np.random.rand(layers_dims[l], layers_dims[l-1]) * 0.01\n",
        "        elif initialization == 'Xavier':\n",
        "            parameters[\"W\"+str(l)]= np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2/(layers_dims[l]+layers_dims[l-1]))\n",
        "        parameters['b' + str(l)] =  np.zeros((layers_dims[l], 1))\n",
        "    return parameters\n",
        "\n",
        "def prev_updates(layers_dims):\n",
        "        previous_updates = {}\n",
        "        L = len(layers_dims)            # number of layers in the network\n",
        "        for l in range(1, L):\n",
        "            previous_updates[\"W\"+str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
        "            previous_updates[\"b\"+str(l)] = np.zeros((layers_dims[l], 1))\n",
        "                    \n",
        "        return previous_updates\n",
        "\n",
        "\n",
        "def feed_forward(A, W, b):\n",
        "\n",
        "    Z =np.dot(W, A) + b\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid(Z):\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "def Relu_derivative(Z):\n",
        "    return 1*(Z>0) \n",
        "\n",
        "def tanh(Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "def tanh_backward(Z):\n",
        "    t = np.tanh(Z)\n",
        "    dt = 1 - (t**2)\n",
        "    return dt\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "   \n",
        "    Z = cache\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) \n",
        "    \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "    \n",
        "\n",
        " \n",
        "def softmax(n):\n",
        " \te = exp(n)\n",
        " \treturn e / e.sum()\n",
        " \t\n",
        " \t\n",
        " \t\n",
        "def activation_forward(A_prev, W, b, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache  = feed_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "\n",
        "    if activation == \"tanh\":\n",
        "        Z, linear_cache  = feed_forward(A_prev, W, b)\n",
        "        A, activation_cache = tanh(Z)\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        \n",
        "        Z, linear_cache = feed_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    \n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"sigmoid\")\n",
        "        caches.append(cache)\n",
        "        \n",
        "    AL, cache = activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation=\"sigmoid\")\n",
        "    caches.append(cache)\n",
        "            \n",
        "    return AL, caches\n",
        "    \n",
        "#backpropagtion    \n",
        "def linear_backward(dZ, cache):\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    dW = 1./m*np.dot(dZ, A_prev.T)\n",
        "    db = 1./m*np.sum(dZ, axis = 1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "    \n",
        "    \n",
        "def activation_backward(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    if activation == \"tanh\":\n",
        "        dZ = tanh_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "        \n",
        "    if activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    \n",
        "    return dA_prev, dW, db    \n",
        "\n",
        "def L_model_backward(Y,AL, caches):\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[\"dA\" + str(l + 2)],  current_cache, activation=\"relu\")\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "def update_parameters(parameters, grads, learning_rate,lamda):\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)] \n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
        "        \n",
        "    return parameters\n",
        "    \n",
        "#stochastic gradient    \n",
        "batch_size = 100\n",
        "iterations_bat = int(train_x.shape[1]/batch_size)   \n",
        "def stochastic_gradient(X, Y, layers_dims, learning_rate,num_epochs,lamda,initialisation):\n",
        "          parameters = initialize_parameters(layers_dims,initialisation)\n",
        "          for j in range(0,num_epochs):\n",
        "            for i in range(0,iterations_bat):\n",
        "                start = i*batch_size\n",
        "                end = start+batch_size\n",
        "                AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "                grads = L_model_backward(Y[:,start:end],AL, caches)\n",
        "                parameters = update_parameters(parameters, grads, learning_rate,lamda)\n",
        "            z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "            z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "            zyy = train_y.flatten()\n",
        "            z_acc = accuracy_score(zyy,z_pred)\n",
        "            print(\"Train accuracy\",z_acc) \n",
        "            z_pred_1, caches = L_model_forward(x_val, parameters)\n",
        "            z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "            zyy = y_val.flatten()\n",
        "            z_acc = accuracy_score(zyy,z_pred)\n",
        "            print(\"validation accuracy\",z_acc) \n",
        "          return parameters.z_acc\n",
        "#momentum gradient descent optimizer\n",
        "def momentum(X,Y,layers_dims,learning_rate,beta,num_epochs,initialisation):\n",
        "    parameters = initialize_parameters(layers_dims,initialisation)\n",
        "    previous_updates =prev_updates(layers_dims)\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "    for j in range(0,num_epochs):\n",
        "        for i in range(0,iterations_bat):\n",
        "            start = i*batch_size\n",
        "            end = start+batch_size\n",
        "            AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "            grads = L_model_backward(Y[:,start:end],AL, caches)\n",
        "                                   \n",
        "            for l in range(1, L + 1):\n",
        "                previous_updates[\"W\"+str(l)] = (beta*previous_updates[\"W\"+str(l)]) + ((1-beta)*grads[\"dW\" + str(l)])\n",
        "                parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - (learning_rate*previous_updates[\"W\"+str(l)])\n",
        "                \n",
        "                previous_updates[\"b\"+str(l)] = (beta*previous_updates[\"b\"+str(l)]) + ((1-beta)*grads[\"db\" + str(l)])\n",
        "                parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - (learning_rate*previous_updates[\"b\"+str(l)])\n",
        "            \n",
        "            \n",
        "        z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = train_y.flatten()\n",
        "        z_acc = accuracy_score(zyy,z_pred)\n",
        "        print(\"Train accuracy\",z_acc) \n",
        "        z_pred_1, caches = L_model_forward(x_val, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = y_val.flatten()\n",
        "        z_acc = accuracy_score(zyy,z_pred)\n",
        "        print(\"validation accuracy\",z_acc) \n",
        "    return parameters, previous_updates,z_acc\n",
        "# rmsprop optimizer\n",
        "def rmsprop(X,Y,layers_dims,learning_rate,beta,num_epochs,initialisation):\n",
        "    parameters = initialize_parameters(layers_dims,initialisation)\n",
        "    previous_updates =prev_updates(layers_dims)\n",
        "    for j in range(0,num_epochs):\n",
        "        for i in range(0,iterations_bat):\n",
        "           \n",
        "           start = i*batch_size\n",
        "           end = start+batch_size\n",
        "           AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "\n",
        "           grads = L_model_backward(AL, Y[:,start:end], caches)\n",
        "\n",
        "           delta = 1e-6 \n",
        "            \n",
        "           L = len(parameters) // 2 \n",
        "        \n",
        "           for l in range(1, L + 1):\n",
        "                vdw = beta*previous_updates[\"W\" + str(l)] + (1-beta)*np.multiply(grads[\"dW\" + str(l)],grads[\"dW\" + str(l)])\n",
        "                vdb = beta*previous_updates[\"b\" + str(l)] + (1-beta)*np.multiply(grads[\"db\" + str(l)],grads[\"db\" + str(l)])\n",
        "        \n",
        "                parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)] / (np.sqrt(vdw)+delta)\n",
        "                parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)] / (np.sqrt(vdb)+delta)\n",
        "        \n",
        "                previous_updates[\"W\" + str(l)] = vdw\n",
        "                previous_updates[\"b\" + str(l)] = vdb\n",
        "             \n",
        "        z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = train_y.flatten()\n",
        "        z_acc = accuracy_score(zyy,z_pred)\n",
        "        print(\"Train accuracy\",z_acc) \n",
        "        z_pred_1, caches = L_model_forward(x_val, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = y_val.flatten()\n",
        "        z_acc = accuracy_score(zyy,z_pred)\n",
        "        print(\"validation accuracy\",z_acc)             \n",
        "    return parameters, previous_updates,z_acc\n",
        "\n",
        "def adam(X,Y,layers_dims,v,m,t,learning_rate,beta,num_epochs,initialisation):\n",
        "    parameters = initialize_parameters(layers_dims,initialisation)\n",
        "    for j in range(0,num_epochs):\n",
        "            for i in range(0,iterations_bat):\n",
        "                start = i*batch_size\n",
        "                end = start+batch_size\n",
        "                AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "                grads = L_model_backward(Y[:,start:end],AL, caches)\n",
        "                \n",
        "                L = len(parameters) // 2 # number of layers in the neural network\n",
        "                beta1 = 0.9\n",
        "                beta2 = 0.999\n",
        "                epsilon = 1e-8\n",
        "            \n",
        "                for l in range(1, L+1):\n",
        "                    mdw = beta1*m[\"W\"+str(l)] + (1-beta1)*grads[\"dW\"+str(l)]\n",
        "                    vdw = beta2*v[\"W\"+str(l)] + (1-beta2)*np.square(grads[\"dW\"+str(l)])\n",
        "                    mw_hat = mdw/(1.0 - beta1**t)\n",
        "                    vw_hat = vdw/(1.0 - beta2**t)\n",
        "            \n",
        "                    parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - (learning_rate * mw_hat)/np.sqrt(vw_hat + epsilon)\n",
        "            \n",
        "                    mdb = beta1*m[\"b\"+str(l)] + (1-beta1)*grads[\"db\"+str(l)]\n",
        "                    vdb = beta2*v[\"b\"+str(l)] + (1-beta2)*np.square(grads[\"db\"+str(l)])\n",
        "                    mb_hat = mdb/(1.0 - beta1**t)\n",
        "                    vb_hat = vdb/(1.0 - beta2**t)\n",
        "            \n",
        "                    parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - (learning_rate * mb_hat)/np.sqrt(vb_hat + epsilon)\n",
        "            \n",
        "                    v[\"dW\"+str(l)] = vdw\n",
        "                    m[\"dW\"+str(l)] = mdw\n",
        "                    v[\"db\"+str(l)] = vdb\n",
        "                    m[\"db\"+str(l)] = mdb\n",
        "            \n",
        "                t = t + 1 # timestep                \n",
        "            z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "            z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "            zyy = train_y.flatten()\n",
        "            z_acc = accuracy_score(zyy,z_pred)\n",
        "            print(\"Train accuracy\",z_acc) \n",
        "            z_pred_1, caches = L_model_forward(x_val, parameters)\n",
        "            z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "            zyy = y_val.flatten()\n",
        "            z_acc = accuracy_score(zyy,z_pred)\n",
        "            print(\"validation accuracy\",z_acc) \n",
        "    return parameters,v,m,t,z_acc\n",
        "\n",
        "def Nadam(X,Y,layers_dims,m,v,t,learning_rate,beta,num_epochs,initialisation):\n",
        "    parameters = initialize_parameters(layers_dims,initialisation)\n",
        "    previous_updates = v\n",
        "    L = len(parameters )//2\n",
        "    for j in range(0,num_epochs):\n",
        "        for l in range(1, L+1):\n",
        "            parameters [\"W\"+str(l)] = parameters [\"W\"+str(l)] - beta*previous_updates[\"W\"+str(l)]\n",
        "            parameters [\"b\"+str(l)] = parameters [\"b\"+str(l)] - beta*previous_updates[\"b\"+str(l)]\n",
        "        for i in range(0,iterations_bat):\n",
        "            start = i*batch_size\n",
        "            end = start+batch_size\n",
        "            AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "            grads = L_model_backward( Y[:,start:end],AL,caches)\n",
        "            \n",
        "            L = len(parameters) // 2 # number of layers in the neural network\n",
        "            beta1 = 0.9\n",
        "            beta2 = 0.999\n",
        "            epsilon = 1e-8\n",
        "        \n",
        "            for l in range(1, L+1):\n",
        "                mdw = beta1*m[\"W\"+str(l)] + (1-beta1)*grads[\"dW\"+str(l)]\n",
        "                vdw = beta2*v[\"W\"+str(l)] + (1-beta2)*np.square(grads[\"dW\"+str(l)])\n",
        "                mw_hat = mdw/(1.0 - beta1**t)\n",
        "                vw_hat = vdw/(1.0 - beta2**t)\n",
        "        \n",
        "                parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - (learning_rate * mw_hat)/np.sqrt(vw_hat + epsilon)\n",
        "        \n",
        "                mdb = beta1*m[\"b\"+str(l)] + (1-beta1)*grads[\"db\"+str(l)]\n",
        "                vdb = beta2*v[\"b\"+str(l)] + (1-beta2)*np.square(grads[\"db\"+str(l)])\n",
        "                mb_hat = mdb/(1.0 - beta1**t)\n",
        "                vb_hat = vdb/(1.0 - beta2**t)\n",
        "        \n",
        "                parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - (learning_rate * mb_hat)/np.sqrt(vb_hat + epsilon)\n",
        "        \n",
        "                v[\"dW\"+str(l)] = vdw\n",
        "                m[\"dW\"+str(l)] = mdw\n",
        "                v[\"db\"+str(l)] = vdb\n",
        "                m[\"db\"+str(l)] = mdb\n",
        "        \n",
        "            t = t + 1 # timestep            \n",
        "\n",
        "        z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = train_y.flatten()\n",
        "        z_acc = accuracy_score(zyy,z_pred)\n",
        "        print(\"Train accuracy\",z_acc) \n",
        "        z_pred_1, caches = L_model_forward(x_val, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = y_val.flatten()\n",
        "        z_acc = accuracy_score(zyy,z_pred)\n",
        "        print(\"validation accuracy\",z_acc) \n",
        "\n",
        "    return parameters,z_acc\n",
        "\n",
        "def nesterov(X,Y,learning_rate,beta,previous_updates,num_epochs,initialisation):\n",
        "        \n",
        "    parameters=initialize_parameters(layers_dims,initialisation)\n",
        "    L = len(parameters)//2\n",
        "    for j in range(0,num_epochs):\n",
        "        for l in range(1, L+1):\n",
        "            parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - beta*previous_updates[\"W\"+str(l)]\n",
        "            parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - beta*previous_updates[\"b\"+str(l)]\n",
        "        for i in range(0,iterations_bat):\n",
        "            start = i*batch_size\n",
        "            end = start+batch_size    \n",
        "            AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "            grads = L_model_backward( Y[:,start:end],AL,caches)\n",
        "            \n",
        "            L = len(parameters) // 2 # number of layers in the neural network\n",
        "           \n",
        "            for l in range(1, L + 1):\n",
        "                previous_updates[\"W\"+str(l)] = beta*previous_updates[\"W\"+str(l)] + (1-beta)*grads[\"dW\" + str(l)]\n",
        "                parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate*previous_updates[\"W\"+str(l)]\n",
        "                \n",
        "                previous_updates[\"b\"+str(l)] = beta*previous_updates[\"b\"+str(l)] + (1-beta)*grads[\"db\" + str(l)]\n",
        "                parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate*previous_updates[\"b\"+str(l)]\n",
        "             \n",
        "        z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = train_y.flatten()\n",
        "        z_acc = accuracy_score(zyy,z_pred)\n",
        "        print(\"Train accuracy\",z_acc) \n",
        "        z_pred_1, caches = L_model_forward(x_val, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = y_val.flatten()\n",
        "        z_acc = accuracy_score(zyy,z_pred)\n",
        "        print(\"validation accuracy\",z_acc) \n",
        "    return parameters,z_acc            \n",
        "\n",
        "def MSE(X,Y,parameters):\n",
        "    AL, caches = L_model_forward(X, parameters)\n",
        "    sm=[]\n",
        "    for i in range(AL.shape[1]):\n",
        "        n=AL[:,i]\n",
        "        u=softmax(n)\n",
        "        sm.append(u)\n",
        "    p=np.array(sm) \n",
        "    v=p.T\n",
        "    Loss = (1/2) * np.sum((Y-v)**2)/train_x.shape[1]\n",
        "    return Loss\n",
        "\n",
        "\n",
        "def cross_entropy_loss(X,Y,parameters):\n",
        "    AL, caches = L_model_forward(X, parameters)\n",
        "    sm=[]\n",
        "    for i in range(AL.shape[1]):\n",
        "        n=AL[:,i]\n",
        "        u=softmax(n)\n",
        "        sm.append(u)\n",
        "    p=np.array(sm) \n",
        "    v=p.T\n",
        "    val=-np.sum(Y*(np.log(v)))\n",
        "    val=val/train_x.shape[1]    \n",
        "    return val\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "3pIuC5dTXotf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_netwrok(learning_rate,num_epochs,optimizer,loss,init_param,hid_layer_sizes):\n",
        "    t = 1\n",
        "    beta = 0.9\n",
        "    lamda = 0.0005\n",
        "    hid_layer_sizes\n",
        "    no_of_class = [len(np.unique(train_y))]\n",
        "    layers_dims = [len(train_x)] + hid_layer_sizes +no_of_class\n",
        "    \n",
        "    print(layers_dims)\n",
        "#    loss = \"cross_entropy\"\n",
        "    initialisation = init_param #\"Xavier\"\n",
        "    gd_optimizer = optimizer\n",
        "    previous_updates = prev_updates(layers_dims)\n",
        "    if(gd_optimizer == \"stochastic_gradient\"):\n",
        "        parameters,z_acc =stochastic_gradient(train_x, Y, layers_dims, learning_rate,num_epochs,lamda,initialisation)\n",
        "        if loss ==\"cross_entropy\" :\n",
        "            Train_loss = cross_entropy_loss(train_x,Y,parameters)\n",
        "            Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "            print(Train_loss)\n",
        "            print(Val_loss)\n",
        "        elif loss ==\"MSE\" :\n",
        "             Train_loss = MSE(train_x,Y,parameters) \n",
        "             Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "             print(Train_loss)\n",
        "             print(Val_loss)\n",
        "    if(gd_optimizer == \"momentum\"):\n",
        "        parameters,previous_updates,z_acc=momentum(train_x,Y,layers_dims,learning_rate,beta,num_epochs,initialisation)\n",
        "        if loss == \"cross_entropy\" :\n",
        "            Train_loss = cross_entropy_loss(train_x,Y,parameters)\n",
        "            Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "            print(Train_loss)\n",
        "            print(Val_loss)\n",
        "        elif loss == \"MSE\" :\n",
        "             Train_loss = MSE(train_x,Y,parameters) \n",
        "             Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "             print(Train_loss)\n",
        "             print(Val_loss)\n",
        "    if(gd_optimizer == \"rmsprop\"):\n",
        "        parameters, previous_updates,z_acc=rmsprop(train_x,Y,layers_dims,learning_rate,beta,num_epochs,initialisation)\n",
        "        if loss == \"cross_entropy\" :\n",
        "            Train_loss = cross_entropy_loss(train_x,Y,parameters)\n",
        "            Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "            print(Train_loss)\n",
        "            print(Val_loss)\n",
        "        elif loss == \"MSE\" :\n",
        "             Train_loss = MSE(train_x,Y,parameters)\n",
        "             Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "             print(Train_loss)\n",
        "             print(Val_loss)\n",
        "    if(gd_optimizer == \"Adam\"):\n",
        "        parameters,v,m,t,z_acc=adam(train_x,Y,layers_dims,previous_updates,previous_updates,t,learning_rate,beta,num_epochs,initialisation)\n",
        "        if loss == \"cross_entropy\" :\n",
        "            Train_loss = cross_entropy_loss(train_x,Y,parameters)\n",
        "            Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "            print(Train_loss)\n",
        "            print(Val_loss)\n",
        "        elif loss == \"MSE\" :\n",
        "             Train_loss = MSE(train_x,Y,parameters) \n",
        "             Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "             print(Train_loss)\n",
        "             print(Val_loss)\n",
        "    if(gd_optimizer == \"Nadam\"): \n",
        "        parameters,z_acc=Nadam(train_x,Y,layers_dims,previous_updates,previous_updates,t,learning_rate,beta,num_epochs,initialisation)\n",
        "        if loss == \"cross_entropy\" :\n",
        "            Train_loss = cross_entropy_loss(train_x,Y,parameters)\n",
        "            Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "            print(Train_loss)\n",
        "            print(Val_loss)\n",
        "        elif loss == \"MSE\" :\n",
        "             Train_loss = MSE(train_x,Y,parameters)\n",
        "             Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "             print(Train_loss)\n",
        "             print(Val_loss)\n",
        "    if(gd_optimizer == \"nesterov\"):\n",
        "        parameters,z_acc=nesterov(train_x,Y,learning_rate,beta,previous_updates,num_epochs,initialisation)\n",
        "        if loss == \"cross_entropy\" :\n",
        "            Train_loss = cross_entropy_loss(train_x,Y,parameters)\n",
        "            Val_loss = cross_entropy_loss(x_val,y_val,parameters)\n",
        "            print(Train_loss)\n",
        "            print(Val_loss)\n",
        "        elif loss == \"MSE\" :\n",
        "             Train_loss = MSE(train_x,Y,parameters)  \n",
        "             Val_loss = cross_entropy_loss(x_val,Y_val,parameters)\n",
        "             print(Train_loss)\n",
        "             print(Val_loss)\n",
        "    accuracy = z_acc * 100\n",
        "    wandb.log({'train_loss':Train_loss})         \n",
        "    wandb.log({'validation_accuracy':accuracy})\n",
        "    wandb.log({'val_loss':Val_loss})\n",
        "    wandb.log({'num_epochs':num_epochs})\n",
        "    return accuracy        \n"
      ],
      "metadata": {
        "id": "Jitq8AAXTSqI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize' \n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {\n",
        "            'values': [0.01,0.001]\n",
        "        },\n",
        "        'num_epoch': {\n",
        "            'values': [5,10,15,20]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['Adam','Nadam','Momentum']\n",
        "        },\n",
        "        'loss': {\n",
        "            'values': ['cross_entropy']\n",
        "        },\n",
        "        'init_param': {\n",
        "            'values': ['Xavier','Normal','Uniform']\n",
        "        },\n",
        "        'hid_layer_sizes': {\n",
        "            'values': [[256,128,64], [128,64,32]]\n",
        "        },               \n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "T67Rlu8caU3j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"Assignment-1\", entity=\"swe-rana\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZVwiEv5ZeJA",
        "outputId": "8172eba9-05fc-46e8-adc2-5a88aeef378f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: tkun9brv\n",
            "Sweep URL: https://wandb.ai/swe-rana/Assignment-1/sweeps/tkun9brv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    config_defaults = {\n",
        "        'learning_rate': 0.01,\n",
        "        'num_epochs': 5,\n",
        "        'optimizer': 'Adam',\n",
        "        'loss': 'cross_entropy',\n",
        "        'init_param': 'Xavier',\n",
        "        'hid_layer_sizes' : [256,128]\n",
        "        \n",
        "    }\n",
        "\n",
        "    wandb.init(config=config_defaults)\n",
        "    config = wandb.config\n",
        "    learning_rate = config.learning_rate\n",
        "    num_epochs = config.num_epochs\n",
        "    optimizer = config.optimizer\n",
        "    loss = config.loss\n",
        "    init_param = config.init_param\n",
        "    hid_layer_sizes = config.hid_layer_sizes\n",
        "    accuracy=train_netwrok(learning_rate,num_epochs,optimizer,loss,init_param,hid_layer_sizes)\n",
        "    \n",
        "\n",
        "#learning_rate = 0.01\n",
        "#num_epochs=30\n",
        "#optimizer = \"Adam\"\n",
        "#hid_layer_sizes = [256,128]\n",
        "#accuracy = train_netwrok(learning_rate,num_epoch,optimizer,loss,init_param,hid_layer_sizes)\n",
        "\n"
      ],
      "metadata": {
        "id": "5gEZuC7jZ1TU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "wandb.agent(sweep_id, train, count=10)"
      ],
      "metadata": {
        "id": "YCwCQCW_bdmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}