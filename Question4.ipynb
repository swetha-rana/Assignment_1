{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b385adf9b2cd41e69e92777b9d77133d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_729fcd3280324553914746ee7f07493a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d4dccbbcb39340038b74317ad986ae57",
              "IPY_MODEL_3c7adc20ab74456fb302c01968137b86"
            ]
          }
        },
        "729fcd3280324553914746ee7f07493a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4dccbbcb39340038b74317ad986ae57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_d979312972d34d689ac050d75fb4ff2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.03MB of 0.03MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_84839c0a21894c29b3c1dfe8f3e59cd7"
          }
        },
        "3c7adc20ab74456fb302c01968137b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f18bcc4d934d43169785a3f04698ff91",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e2e0f71ce354319ba833cf610af1152"
          }
        },
        "d979312972d34d689ac050d75fb4ff2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84839c0a21894c29b3c1dfe8f3e59cd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f18bcc4d934d43169785a3f04698ff91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e2e0f71ce354319ba833cf610af1152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhnKzTaVgKRx",
        "outputId": "eeaadf68-24a6-4e7f-e7ca-2bc945fd300a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.10)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.6)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"Assignment-1\", entity=\"swe-rana\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "FmouXlF5gt-S",
        "outputId": "fd6d570a-8e87-482d-aabe-6b47c492f56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mswe-rana\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/swe-rana/Assignment-1/runs/h5nq2eqj\" target=\"_blank\">grateful-glade-187</a></strong> to <a href=\"https://wandb.ai/swe-rana/Assignment-1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fbfc2478a10>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/swe-rana/Assignment-1/runs/h5nq2eqj?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"Assignment-1\", entity=\"swe-rana\")\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "(X_train,y_train),(X_test,y_test) = fashion_mnist.load_data()\n",
        "label0=[]\n",
        "label1=[]\n",
        "label2=[]\n",
        "label3=[]\n",
        "label4=[]\n",
        "label5=[]\n",
        "label6=[]\n",
        "label7=[]\n",
        "label8=[]\n",
        "label9=[]\n",
        "for i in range(len(y_train)):\n",
        "    if (y_train[i]==0):\n",
        "        label0.append(i)\n",
        "    if (y_train[i]==1):\n",
        "        label1.append(i)\n",
        "    if (y_train[i]==2):\n",
        "        label2.append(i)\n",
        "    if (y_train[i]==3):\n",
        "        label3.append(i)\n",
        "    if (y_train[i]==4):\n",
        "        label4.append(i)\n",
        "    if (y_train[i]==5):\n",
        "        label5.append(i)\n",
        "    if (y_train[i]==6):\n",
        "        label6.append(i)\n",
        "    if (y_train[i]==7):\n",
        "        label7.append(i)\n",
        "    if (y_train[i]==8):\n",
        "        label8.append(i)\n",
        "    if (y_train[i]==9):\n",
        "        label9.append(i)\n",
        "Class_names=(label0,label1,label2,label3,label4,label5,label6,label7,label8,label9)    \n",
        "data = (\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\")\n",
        "rows=2\n",
        "columns =5\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "for i,j in zip(range(1, columns*rows +1),range(0,10)):\n",
        "        num = random.choice(Class_names[j])\n",
        "        wandb.log({\"images\": [wandb.Image(X_train[num],caption=data[j])]})\n",
        "        fig.add_subplot(rows, columns, i)\n",
        "        plt.imshow(X_train[num],cmap =\"gray\")\n",
        "        plt.axis('off')\n",
        "        plt.title(data[j])\n",
        "wandb.log({\"chart\": plt})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "b385adf9b2cd41e69e92777b9d77133d",
            "729fcd3280324553914746ee7f07493a",
            "d4dccbbcb39340038b74317ad986ae57",
            "3c7adc20ab74456fb302c01968137b86",
            "d979312972d34d689ac050d75fb4ff2c",
            "84839c0a21894c29b3c1dfe8f3e59cd7",
            "f18bcc4d934d43169785a3f04698ff91",
            "1e2e0f71ce354319ba833cf610af1152"
          ]
        },
        "id": "UBGY9t2ehGvq",
        "outputId": "a24a14e2-3ee1-4c1f-952f-45e519571bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:3orq3prt) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 2237... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b385adf9b2cd41e69e92777b9d77133d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">sleek-darkness-188</strong>: <a href=\"https://wandb.ai/swe-rana/Assignment-1/runs/3orq3prt\" target=\"_blank\">https://wandb.ai/swe-rana/Assignment-1/runs/3orq3prt</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220225_175827-3orq3prt/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:3orq3prt). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/swe-rana/Assignment-1/runs/31o1291t\" target=\"_blank\">charmed-armadillo-189</a></strong> to <a href=\"https://wandb.ai/swe-rana/Assignment-1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import exp\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "(t_train_x_orig,t_train_y),(test_x_orig,test_y)= fashion_mnist.load_data()\n",
        "\n",
        "train_x_orig,x_val,train_y,y_val=train_test_split(t_train_x_orig,t_train_y,test_size=0.1,random_state = 43)\n",
        "\n",
        "m_train = train_x_orig.shape[0]\n",
        "num_px = train_x_orig.shape[1]\n",
        "m_test = test_x_orig.shape[0]\n",
        "train_y = train_y.reshape(1, len(train_y))\n",
        "test_y = test_y.reshape(1, len(test_y))\n",
        "y_val = y_val.reshape(1, len(y_val))\n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
        "x_val_flatten = x_val.reshape(x_val.shape[0],-1).T\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "no_of_class=10\n",
        "\n",
        "train_x = train_x_flatten/255\n",
        "test_x = test_x_flatten/255\n",
        "x_val = x_val_flatten/255\n",
        "#layers_dims = [len(train_x),256,128,no_of_class]\n",
        "onehot_encoded = list()\n",
        "\n",
        "for i in range(train_y.shape[1]):\n",
        "    c=train_y[:,i][0]\n",
        "    letter = [0 for _ in range(no_of_class)]\n",
        "    letter[c] = 1\n",
        "    onehot_encoded.append(letter)\n",
        "\n",
        "N=np.array(onehot_encoded)\n",
        "Y=N.reshape(no_of_class,train_y.shape[1])\n",
        "for i in range(0,train_y.shape[1]):\n",
        "      Y[:,i] = N[i]\n",
        "\n",
        "onehot_encoded_y_val = list()\n",
        "\n",
        "for i in range(y_val.shape[1]):\n",
        "    c=y_val[:,i][0]\n",
        "    letter = [0 for _ in range(no_of_class)]\n",
        "    letter[c] = 1\n",
        "    onehot_encoded_y_val.append(letter)\n",
        "\n",
        "\n",
        "M=np.array(onehot_encoded_y_val)\n",
        "Y_val=M.reshape(no_of_class,y_val.shape[1])\n",
        "for i in range(0,y_val.shape[1]):\n",
        "      Y_val[:,i] = N[i]\n",
        "\n",
        "\n",
        "#layers_dims = [len(train_x),256,128,no_of_class]\n",
        "\n",
        "\n",
        "def initialize_parameters(layers_dims,initialization):    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layers_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        if initialization == 'Normal':\n",
        "            parameters[\"W\"+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01\n",
        "        elif initialization == 'Uniform':\n",
        "            parameters[\"W\"+str(l)] = np.random.rand(layers_dims[l], layers_dims[l-1]) * 0.01\n",
        "        elif initialization == 'Xavier':\n",
        "            parameters[\"W\"+str(l)]= np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2/(layers_dims[l]+layers_dims[l-1]))\n",
        "        parameters['b' + str(l)] =  np.zeros((layers_dims[l], 1))\n",
        "    return parameters\n",
        "\n",
        "def prev_updates(layers_dims):\n",
        "        previous_updates = {}\n",
        "        L = len(layers_dims)            # number of layers in the network\n",
        "        for l in range(1, L):\n",
        "            previous_updates[\"W\"+str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
        "            previous_updates[\"b\"+str(l)] = np.zeros((layers_dims[l], 1))\n",
        "                    \n",
        "        return previous_updates\n",
        "\n",
        "\n",
        "def feed_forward(A, W, b):\n",
        "\n",
        "    Z =np.dot(W, A) + b\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid(Z):\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def softmax(Z):\n",
        "    A = exp(Z)\n",
        "    A = A/A.sum()\n",
        "    return A\n",
        "\n",
        "def relu(Z):\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "def tanh(Z):\n",
        "    A = np.tanh(Z)\n",
        "    cache = Z\n",
        "    return A,cache\n",
        "\n",
        "def tanh_backward(dA,cache):\n",
        "    Z=cache\n",
        "    t = np.tanh(Z)\n",
        "    dZ = 1 - (t**2)\n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "   \n",
        "    Z = cache\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) \n",
        "    \n",
        "    dZ[Z <= 0] = 0\n",
        "     \n",
        "    return dZ\n",
        "    \n",
        " \t\n",
        " \t\n",
        " \t\n",
        "def activation_forward(A_prev, W, b, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache  = feed_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "\n",
        "    if activation == \"tanh\":\n",
        "        Z, linear_cache  = feed_forward(A_prev, W, b)\n",
        "        A, activation_cache = tanh(Z)\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        \n",
        "        Z, linear_cache = feed_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    \n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation)\n",
        "        caches.append(cache)\n",
        "        \n",
        "    AL, cache = activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation)\n",
        "    caches.append(cache)\n",
        "    AP=[]
        "    for i in range(AL.shape[1]):\n",
        "                AA = softmax(AL[:,i])\n",
        "                AP.append(AA)       \n",
        "    return AL, caches\n",
        "    \n",
        "#backpropagtion    \n",
        "def linear_backward(dZ, cache):\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    dW = 1./m*np.dot(dZ, A_prev.T)\n",
        "    db = 1./m*np.sum(dZ, axis = 1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "    \n",
        "    \n",
        "def activation_backward(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    if activation == \"tanh\":\n",
        "        dZ = tanh_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "        \n",
        "    if activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    \n",
        "    return dA_prev, dW, db    \n",
        "\n",
        "def L_model_backward(Y,AL, caches):\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = activation_backward(dAL, current_cache, activation)\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[\"dA\" + str(l + 2)],  current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "def update_parameters(parameters, grads, learning_rate,lamda):\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)] \n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
        "        \n",
        "    return parameters\n",
        "    \n",
        "#stochastic gradient    \n",
        "\n",
        "\n",
        "def stochastic_gradient(X, Y, layers_dims, learning_rate,num_epochs,lamda,initialisation,loss):\n",
        "          z_acc = []\n",
        "          val_acc = []\n",
        "          Train_loss = []\n",
        "          Val_loss = []\n",
        "          parameters = initialize_parameters(layers_dims,initialisation)\n",
        "          print(\"in stochastic\")\n",
        "          for j in range(0,num_epochs):\n",
        "            for i in range(0,iterations_bat):\n",
        "                start = i*batch_size\n",
        "                end = start+batch_size\n",
        "                AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "                grads = L_model_backward(Y[:,start:end],AL, caches)\n",
        "                parameters = update_parameters(parameters, grads, learning_rate,lamda)\n",
        "            z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "            z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "            zyy = train_y.flatten()\n",
        "            z_acc.append(accuracy_score(zyy,z_pred))\n",
        "            print(\"Train accuracy\",z_acc) \n",
        "            val_pred, caches = L_model_forward(x_val, parameters)\n",
        "            val_prediction = np.argmax(val_pred,axis = 0)\n",
        "            val_y_flat = y_val.flatten()\n",
        "            val_acc.append(accuracy_score(val_y_flat,val_prediction))\n",
        "\n",
        "            if loss ==\"cross_entropy\" :\n",
        "                Train_loss.append(cross_entropy_loss(train_x,Y,parameters))\n",
        "                Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "            elif loss ==\"MSE\" :\n",
        "                 Train_loss.append(MSE(train_x,Y,parameters))\n",
        "                 Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "          return parameters,z_acc,val_acc,Train_loss,Val_loss\n",
        "#momentum gradient descent optimizer\n",
        "def momentum(X,Y,layers_dims,learning_rate,beta,num_epochs,initialisation,loss):\n",
        "    z_acc = []\n",
        "    val_acc = []\n",
        "    \n",
        "    Train_loss = []\n",
        "    Val_loss = []\n",
        "    parameters = initialize_parameters(layers_dims,initialisation)\n",
        "    previous_updates =prev_updates(layers_dims)\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "    for j in range(0,num_epochs):\n",
        "        for i in range(0,iterations_bat):\n",
        "            start = i*batch_size\n",
        "            end = start+batch_size\n",
        "            AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "            grads = L_model_backward(Y[:,start:end],AL, caches)\n",
        "                                   \n",
        "            for l in range(1, L + 1):\n",
        "                previous_updates[\"W\"+str(l)] = (beta*previous_updates[\"W\"+str(l)]) + ((1-beta)*grads[\"dW\" + str(l)])\n",
        "                parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - (learning_rate*previous_updates[\"W\"+str(l)])\n",
        "                \n",
        "                previous_updates[\"b\"+str(l)] = (beta*previous_updates[\"b\"+str(l)]) + ((1-beta)*grads[\"db\" + str(l)])\n",
        "                parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - (learning_rate*previous_updates[\"b\"+str(l)])\n",
        "            \n",
        "          \n",
        "        z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = train_y.flatten()\n",
        "        z_acc.append(accuracy_score(zyy,z_pred))\n",
        "        print(\"Train accuracy\",z_acc) \n",
        "        val_pred, caches = L_model_forward(x_val, parameters)\n",
        "        val_prediction = np.argmax(val_pred,axis = 0)\n",
        "        val_y_flat = y_val.flatten()\n",
        "        val_acc.append(accuracy_score(val_y_flat,val_prediction))\n",
        "        print(\"validation accuracy\",val_acc) \n",
        "        if loss ==\"cross_entropy\" :\n",
        "            Train_loss.append(cross_entropy_loss(train_x,Y,parameters))\n",
        "            Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "        elif loss ==\"MSE\" :\n",
        "             Train_loss.append(MSE(train_x,Y,parameters))\n",
        "             Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "\n",
        "    return parameters, previous_updates,z_acc,val_acc,Train_loss,Val_loss\n",
        "# rmsprop optimizer\n",
        "def rmsprop(X,Y,layers_dims,learning_rate,beta,num_epochs,initialisation,loss):\n",
        "    z_acc = []\n",
        "    val_acc = []\n",
        "    Train_loss = []\n",
        "    Val_loss = []\n",
        "    \n",
        "    parameters = initialize_parameters(layers_dims,initialisation)\n",
        "    previous_updates =prev_updates(layers_dims)\n",
        "    for j in range(0,num_epochs):\n",
        "        for i in range(0,iterations_bat):\n",
        "           \n",
        "           start = i*batch_size\n",
        "           end = start+batch_size\n",
        "           AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "\n",
        "           grads = L_model_backward(AL, Y[:,start:end], caches)\n",
        "\n",
        "           delta = 1e-6 \n",
        "            \n",
        "           L = len(parameters) // 2 \n",
        "        \n",
        "           for l in range(1, L + 1):\n",
        "                vdw = beta*previous_updates[\"W\" + str(l)] + (1-beta)*np.multiply(grads[\"dW\" + str(l)],grads[\"dW\" + str(l)])\n",
        "                vdb = beta*previous_updates[\"b\" + str(l)] + (1-beta)*np.multiply(grads[\"db\" + str(l)],grads[\"db\" + str(l)])\n",
        "        \n",
        "                parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)] / (np.sqrt(vdw)+delta)\n",
        "                parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)] / (np.sqrt(vdb)+delta)\n",
        "        \n",
        "                previous_updates[\"W\" + str(l)] = vdw\n",
        "                previous_updates[\"b\" + str(l)] = vdb\n",
        "           \n",
        "        z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = train_y.flatten()\n",
        "        z_acc.append(accuracy_score(zyy,z_pred))\n",
        "        print(\"Train accuracy\",z_acc) \n",
        "        val_pred, caches = L_model_forward(x_val, parameters)\n",
        "        val_prediction = np.argmax(val_pred,axis = 0)\n",
        "        val_y_flat = y_val.flatten()\n",
        "        val_acc.append(accuracy_score(val_y_flat,val_prediction))\n",
        "        print(\"validation accuracy\",val_acc) \n",
        "        if loss ==\"cross_entropy\" :\n",
        "            Train_loss.append(cross_entropy_loss(train_x,Y,parameters))\n",
        "            Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "        elif loss ==\"MSE\" :\n",
        "             Train_loss.append(MSE(train_x,Y,parameters))\n",
        "             Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))        \n",
        "\n",
        "    return parameters, previous_updates,z_acc,val_acc,Train_loss,Val_loss\n",
        "\n",
        "def adam(X,Y,layers_dims,v,m,t,learning_rate,beta,num_epochs,initialisation,loss):\n",
        "    z_acc = []\n",
        "    val_acc = []\n",
        "    Train_loss = []\n",
        "    Val_loss = []\n",
        "\n",
        "    parameters = initialize_parameters(layers_dims,initialisation)\n",
        "    for j in range(0,num_epochs):\n",
        "            for i in range(0,iterations_bat):\n",
        "                start = i*batch_size\n",
        "                end = start+batch_size\n",
        "                AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "                grads = L_model_backward(Y[:,start:end],AL, caches)\n",
        "                \n",
        "                L = len(parameters) // 2 # number of layers in the neural network\n",
        "                beta1 = 0.9\n",
        "                beta2 = 0.999\n",
        "                epsilon = 1e-8\n",
        "            \n",
        "                for l in range(1, L+1):\n",
        "                    mdw = beta1*m[\"W\"+str(l)] + (1-beta1)*grads[\"dW\"+str(l)]\n",
        "                    vdw = beta2*v[\"W\"+str(l)] + (1-beta2)*np.square(grads[\"dW\"+str(l)])\n",
        "                    mw_hat = mdw/(1.0 - beta1**t)\n",
        "                    vw_hat = vdw/(1.0 - beta2**t)\n",
        "            \n",
        "                    parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - (learning_rate * mw_hat)/np.sqrt(vw_hat + epsilon)\n",
        "            \n",
        "                    mdb = beta1*m[\"b\"+str(l)] + (1-beta1)*grads[\"db\"+str(l)]\n",
        "                    vdb = beta2*v[\"b\"+str(l)] + (1-beta2)*np.square(grads[\"db\"+str(l)])\n",
        "                    mb_hat = mdb/(1.0 - beta1**t)\n",
        "                    vb_hat = vdb/(1.0 - beta2**t)\n",
        "            \n",
        "                    parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - (learning_rate * mb_hat)/np.sqrt(vb_hat + epsilon)\n",
        "            \n",
        "                    v[\"dW\"+str(l)] = vdw\n",
        "                    m[\"dW\"+str(l)] = mdw\n",
        "                    v[\"db\"+str(l)] = vdb\n",
        "                    m[\"db\"+str(l)] = mdb\n",
        "            \n",
        "                t = t + 1 # timestep   \n",
        "            \n",
        "            z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "            z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "            zyy = train_y.flatten()\n",
        "            z_acc.append(accuracy_score(zyy,z_pred))\n",
        "            print(\"Train accuracy\",z_acc) \n",
        "            val_pred, caches = L_model_forward(x_val, parameters)\n",
        "            val_prediction = np.argmax(val_pred,axis = 0)\n",
        "            val_y_flat = y_val.flatten()\n",
        "            val_acc.append(accuracy_score(val_y_flat,val_prediction))\n",
        "            print(\"validation accuracy\",val_acc) \n",
        "\n",
        "            if loss ==\"cross_entropy\" :\n",
        "                Train_loss.append(cross_entropy_loss(train_x,Y,parameters))\n",
        "                Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "            elif loss ==\"MSE\" :\n",
        "                 Train_loss.append(MSE(train_x,Y,parameters))\n",
        "                 Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "\n",
        "    return parameters,v,m,t,z_acc,val_acc,Train_loss,Val_loss\n",
        "\n",
        "def Nadam(X,Y,layers_dims,m,v,t,learning_rate,beta,num_epochs,initialisation,loss):\n",
        "    \n",
        "    z_acc = []\n",
        "    val_acc = []\n",
        "    Train_loss = []\n",
        "    Val_loss = []\n",
        "\n",
        "\n",
        "    parameters = initialize_parameters(layers_dims,initialisation)\n",
        "    previous_updates = v\n",
        "    L = len(parameters )//2\n",
        "    for j in range(0,num_epochs):\n",
        "        for l in range(1, L+1):\n",
        "            parameters [\"W\"+str(l)] = parameters [\"W\"+str(l)] - beta*previous_updates[\"W\"+str(l)]\n",
        "            parameters [\"b\"+str(l)] = parameters [\"b\"+str(l)] - beta*previous_updates[\"b\"+str(l)]\n",
        "        for i in range(0,iterations_bat):\n",
        "            start = i*batch_size\n",
        "            end = start+batch_size\n",
        "            AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "            grads = L_model_backward( Y[:,start:end],AL,caches)\n",
        "            \n",
        "            L = len(parameters) // 2 # number of layers in the neural network\n",
        "            beta1 = 0.9\n",
        "            beta2 = 0.999\n",
        "            epsilon = 1e-8\n",
        "        \n",
        "            for l in range(1, L+1):\n",
        "                mdw = beta1*m[\"W\"+str(l)] + (1-beta1)*grads[\"dW\"+str(l)]\n",
        "                vdw = beta2*v[\"W\"+str(l)] + (1-beta2)*np.square(grads[\"dW\"+str(l)])\n",
        "                mw_hat = mdw/(1.0 - beta1**t)\n",
        "                vw_hat = vdw/(1.0 - beta2**t)\n",
        "        \n",
        "                parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - (learning_rate * mw_hat)/np.sqrt(vw_hat + epsilon)\n",
        "        \n",
        "                mdb = beta1*m[\"b\"+str(l)] + (1-beta1)*grads[\"db\"+str(l)]\n",
        "                vdb = beta2*v[\"b\"+str(l)] + (1-beta2)*np.square(grads[\"db\"+str(l)])\n",
        "                mb_hat = mdb/(1.0 - beta1**t)\n",
        "                vb_hat = vdb/(1.0 - beta2**t)\n",
        "        \n",
        "                parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - (learning_rate * mb_hat)/np.sqrt(vb_hat + epsilon)\n",
        "        \n",
        "                v[\"dW\"+str(l)] = vdw\n",
        "                m[\"dW\"+str(l)] = mdw\n",
        "                v[\"db\"+str(l)] = vdb\n",
        "                m[\"db\"+str(l)] = mdb\n",
        "        \n",
        "            t = t + 1 # timestep            \n",
        "\n",
        "        z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = train_y.flatten()\n",
        "        z_acc.append(accuracy_score(zyy,z_pred))\n",
        "        print(\"Train accuracy\",z_acc) \n",
        "        val_pred, caches = L_model_forward(x_val, parameters)\n",
        "        val_prediction = np.argmax(val_pred,axis = 0)\n",
        "        val_y_flat = y_val.flatten()\n",
        "        val_acc.append(accuracy_score(val_y_flat,val_prediction))\n",
        "        print(\"validation accuracy\",val_acc) \n",
        "        if loss ==\"cross_entropy\" :\n",
        "            Train_loss.append(cross_entropy_loss(train_x,Y,parameters))\n",
        "            Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "        elif loss ==\"MSE\" :\n",
        "             Train_loss.append(MSE(train_x,Y,parameters))\n",
        "             Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "\n",
        "    return parameters,z_acc,val_acc,Train_loss,Val_loss\n",
        "\n",
        "def nesterov(X,Y,learning_rate,beta,previous_updates,num_epochs,initialisation,loss):\n",
        "    z_acc = []\n",
        "    val_acc = []        \n",
        "    Train_loss = []\n",
        "    Val_loss = []\n",
        "\n",
        "    parameters=initialize_parameters(layers_dims,initialisation)\n",
        "    L = len(parameters)//2\n",
        "    for j in range(0,num_epochs):\n",
        "        for l in range(1, L+1):\n",
        "            parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - beta*previous_updates[\"W\"+str(l)]\n",
        "            parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - beta*previous_updates[\"b\"+str(l)]\n",
        "        for i in range(0,iterations_bat):\n",
        "            start = i*batch_size\n",
        "            end = start+batch_size    \n",
        "            AL, caches = L_model_forward(X[:,start:end], parameters)\n",
        "            grads = L_model_backward( Y[:,start:end],AL,caches)\n",
        "            \n",
        "            L = len(parameters) // 2 # number of layers in the neural network\n",
        "           \n",
        "            for l in range(1, L + 1):\n",
        "                previous_updates[\"W\"+str(l)] = beta*previous_updates[\"W\"+str(l)] + (1-beta)*grads[\"dW\" + str(l)]\n",
        "                parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate*previous_updates[\"W\"+str(l)]\n",
        "                \n",
        "                previous_updates[\"b\"+str(l)] = beta*previous_updates[\"b\"+str(l)] + (1-beta)*grads[\"db\" + str(l)]\n",
        "                parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate*previous_updates[\"b\"+str(l)]\n",
        "             \n",
        "        z_pred_1, caches = L_model_forward(train_x, parameters)\n",
        "        z_pred = np.argmax(z_pred_1,axis = 0)\n",
        "        zyy = train_y.flatten()\n",
        "        z_acc.append(accuracy_score(zyy,z_pred))\n",
        "        print(\"Train accuracy\",z_acc) \n",
        "        val_pred, caches = L_model_forward(x_val, parameters)\n",
        "        val_prediction = np.argmax(val_pred,axis = 0)\n",
        "        val_y_flat = y_val.flatten()\n",
        "        val_acc.append(accuracy_score(val_y_flat,val_prediction))\n",
        "        print(\"validation accuracy\",val_acc) \n",
        "        \n",
        "        if loss ==\"cross_entropy\" :\n",
        "            Train_loss.append(cross_entropy_loss(train_x,Y,parameters))\n",
        "            Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))\n",
        "        elif loss ==\"MSE\" :\n",
        "             Train_loss.append(MSE(train_x,Y,parameters))\n",
        "             Val_loss.append(cross_entropy_loss(x_val,Y_val,parameters))        \n",
        "\n",
        "    return parameters,z_acc,val_acc,Train_loss,Val_loss            \n",
        "\n",
        "\n",
        "def MSE(X,Y,parameters):\n",
        "    AL, caches = L_model_forward(X, parameters)\n",
        "    sm=[]\n",
        "    for i in range(AL.shape[1]):\n",
        "        n=AL[:,i]\n",
        "        u=softmax(n)\n",
        "        sm.append(u)\n",
        "    p=np.array(sm) \n",
        "    v=p.T\n",
        "    Loss = (1/2) * np.sum((Y-v)**2)/train_x.shape[1]\n",
        "    return Loss\n",
        "\n",
        "\n",
        "def cross_entropy_loss(X,Y,parameters):\n",
        "    AL, caches = L_model_forward(X, parameters)\n",
        "    sm=[]\n",
        "    for i in range(AL.shape[1]):\n",
        "        n=AL[:,i]\n",
        "        u=softmax(n)\n",
        "        sm.append(u)\n",
        "    p=np.array(sm) \n",
        "    v=p.T\n",
        "    val=-np.sum(Y*(np.log(v)))\n",
        "    val=val/train_x.shape[1]    \n",
        "    return val\n",
        "    \n",
        "global batch_size\n",
        "global iterations_bat\n",
        "global activation\n",
        "global layers_dims\n",
        "    "
      ],
      "metadata": {
        "id": "3pIuC5dTXotf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network(learning_rate,num_epochs,optimizer,loss,init_param,hid_layer_sizes,bat_size,activate_func):\n",
        "    t = 1\n",
        "    beta = 0.9\n",
        "    lamda = 0.0005\n",
        "    hid_layer_sizes\n",
        "    no_of_class = [len(np.unique(train_y))]\n",
        "    global layers_dims\n",
        "    layers_dims = [len(train_x)] + hid_layer_sizes +no_of_class\n",
        "    global activation\n",
        "    global batch_size\n",
        "    global iterations_bat\n",
        "    activation = activate_func\n",
        "    batch_size = bat_size\n",
        "    iterations_bat = int(train_x.shape[1]/batch_size) \n",
        "\n",
        "    print(layers_dims)\n",
        "#    loss = \"cross_entropy\"\n",
        "    initialisation = init_param #\"Xavier\"\n",
        "    gd_optimizer = optimizer\n",
        "    previous_updates = prev_updates(layers_dims)\n",
        "    if(gd_optimizer == \"stochastic_gradient\"):\n",
        "        parameters,z_acc,val_acc,Train_loss,Val_loss =stochastic_gradient(train_x, Y, layers_dims, learning_rate,num_epochs,lamda,initialisation,loss)\n",
        "\n",
        "    if(gd_optimizer == \"momentum\"):\n",
        "        parameters,previous_updates,z_acc,val_acc,Train_loss,Val_loss=momentum(train_x,Y,layers_dims,learning_rate,beta,num_epochs,initialisation,loss)\n",
        "\n",
        "    if(gd_optimizer == \"rmsprop\"):\n",
        "        parameters, previous_updates,z_acc,val_acc,Train_loss,Val_loss=rmsprop(train_x,Y,layers_dims,learning_rate,beta,num_epochs,initialisation,loss)\n",
        "\n",
        "    if(gd_optimizer == \"Adam\"):\n",
        "        parameters,v,m,t,z_acc,val_acc,Train_loss,Val_loss=adam(train_x,Y,layers_dims,previous_updates,previous_updates,t,learning_rate,beta,num_epochs,initialisation,loss)\n",
        "\n",
        "    if(gd_optimizer == \"Nadam\"): \n",
        "        parameters,z_acc,val_acc,Train_loss,Val_loss=Nadam(train_x,Y,layers_dims,previous_updates,previous_updates,t,learning_rate,beta,num_epochs,initialisation,loss)\n",
        "\n",
        "    if(gd_optimizer == \"nesterov\"):\n",
        "        parameters,z_acc,val_acc,Train_loss,Val_loss=nesterov(train_x,Y,learning_rate,beta,previous_updates,num_epochs,initialisation,loss)\n",
        "\n",
        "    train_accuracy = [element * 100 for element in z_acc]\n",
        "    val_accuracy = [element * 100 for element in val_acc]\n",
        "    # wandb.log({'train_loss':Train_loss})         \n",
        "    # wandb.log({'train_accuracy':train_accuracy})\n",
        "    # wandb.log({'val_accuracy':val_accuracy})\n",
        "    # wandb.log({'val_loss':Val_loss})\n",
        "    # wandb.log({'num_epochs':num_epochs})\n",
        "    #for zi in range(len(train_accuracy)):\n",
        "    #    print(zi)\n",
        "    #    wandb.log({\"train_acc\": train_accuracy, \"validation_accuracy\": val_accuracy, \"train_loss\": Train_loss, \"validation cost\": Val_loss, 'epoch': zi+1})\n",
        "\n",
        "    print(\"val accuracy\", val_accuracy)\n",
        "    print(\"train accuracy\", train_accuracy)\n",
        "    return val_accuracy,val_accuracy,train_accuracy,Train_loss,Val_loss        \n",
        "\n",
        "#train_netwrok(0.001,5,\"Adam\",\"cross-entropy\",\"Xavier\",[256,128],100,\"sigmoid\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Jitq8AAXTSqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize' \n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {\n",
        "            'values': [0.01,0.001]\n",
        "        },\n",
        "        'num_epochs': {\n",
        "            'values': [10,15,20]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['Adam','Nadam','momentum','stochastic_gradient','rmsprop','nesterov']\n",
        "        },\n",
        "        'loss': {\n",
        "            'values': ['cross_entropy','MSE']\n",
        "        },\n",
        "        'init_param': {\n",
        "            'values': ['Xavier','Normal']\n",
        "        },\n",
        "        'hid_layer_sizes': {\n",
        "            'values': [[256,128], [256,128,64],[128,128,128]]\n",
        "        }, \n",
        "        'bat_size': {\n",
        "            'values': [16,32,48]\n",
        "        }, \n",
        "        'activate_func': {\n",
        "            'values': ['sigmoid','relu','tanh']\n",
        "        }, \n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "T67Rlu8caU3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sweep_id = wandb.sweep(sweep_config, project=\"Assignment-1\", entity=\"swe-rana\")"
      ],
      "metadata": {
        "id": "wZVwiEv5ZeJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    config_defaults = {\n",
        "        'learning_rate': 0.01,\n",
        "        'num_epochs': 5,\n",
        "        'optimizer': 'Adam',\n",
        "        'loss': 'cross_entropy',\n",
        "        'init_param': 'Xavier',\n",
        "        'hid_layer_sizes' : [256,128],\n",
        "        'bat_size':50,\n",
        "        'activate_func': 'sigmoid'\n",
        "    }\n",
        "\n",
        "    wandb.init(config=config_defaults)\n",
        "    config = wandb.config\n",
        "    learning_rate = config.learning_rate\n",
        "    num_epochs = config.num_epochs\n",
        "    optimizer = config.optimizer\n",
        "    loss = config.loss\n",
        "    init_param = config.init_param\n",
        "    hid_layer_sizes = config.hid_layer_sizes\n",
        "    bat_size = config.bat_size\n",
        "    activate_func = config.activate_func\n",
        "    run_name = \"hl_{}_bs_{}_ac_{}_initparam_{}_op_{}_ep_{}_lr_{}\".format(hid_layer_sizes,bat_size, activate_func, init_param, optimizer,num_epochs,learning_rate)\n",
        "\n",
        "    val_accuracy,val_accuracy,train_accuracy,Train_loss,Val_loss=train_network(learning_rate,num_epochs,optimizer,loss,init_param,hid_layer_sizes,bat_size,activate_func)\n",
        "    for zi in range(len(train_accuracy)):\n",
        "        print(zi)\n",
        "        wandb.log({'train_loss':Train_loss[zi]})         \n",
        "        wandb.log({'train_accuracy':train_accuracy[zi]})\n",
        "        wandb.log({'val_accuracy':val_accuracy[zi]})\n",
        "        wandb.log({'val_loss':Val_loss[zi]})\n",
        "        wandb.log({'num_epochs':zi+1})\n",
        "        #wandb.log({\"train_acc\": train_accuracy, \"validation_accuracy\": val_accuracy, \"train_loss\": Train_loss, \"validation cost\": Val_loss, 'epoch': zi+1})\n",
        "\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "    wandb.run.finish()\n"
      ],
      "metadata": {
        "id": "5gEZuC7jZ1TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Assignment-1\", entity=\"swe-rana\")\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "wandb.agent(sweep_id, train, count=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EsOhzoCi2SB",
        "outputId": "31feed0b-4d8d-47c0-d3e9-14e1426da1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation accuracy [0.09633333333333334, 0.09633333333333334, 0.09633333333333334, 0.09633333333333334, 0.09633333333333334, 0.09633333333333334]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    }
  ]
}
